{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0620c098-b5d0-479e-99bc-6ed61811791d",
   "metadata": {},
   "source": [
    "## Assignment on Logistic Regression 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0936c115-40af-437c-ba40-ab19a5836c8f",
   "metadata": {},
   "source": [
    "Q1. What is the purpose of grid search cv in machine learning, and how does it work?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fd5d76-09a6-4c7a-95b5-69b8b1af4da6",
   "metadata": {},
   "source": [
    "The purpose of Grid Search Cross-Validation (Grid Search CV) in machine learning is to find the optimal combination of hyperparameters for a given model. Hyperparameters are parameters that are not learned from the data but are set by the user before the learning process begins.\n",
    "\n",
    "Grid Search CV works by exhaustively searching through a predefined grid of hyperparameter values and evaluating the model's performance using cross-validation. Here's how it works:\n",
    "\n",
    "Define the Hyperparameter Grid: Specify the hyperparameters to be tuned and the range of values for each hyperparameter. For example, in a decision tree classifier, the hyperparameters could include the maximum depth, the minimum number of samples required to split a node, and the criterion for splitting. The range of values for each hyperparameter is specified as a list or range.\n",
    "\n",
    "Cross-Validation: Divide the training data into multiple subsets or folds. For each hyperparameter combination, perform k-fold cross-validation, where the model is trained on k-1 folds and evaluated on the remaining fold. This is done to estimate the model's performance on unseen data.\n",
    "\n",
    "Model Training and Evaluation: For each hyperparameter combination, train the model using the training data and evaluate its performance on the validation fold or the average performance across all folds. The evaluation metric, such as accuracy, F1 score, or mean squared error, is used to assess the model's performance.\n",
    "\n",
    "Grid Search: Iterate through all possible combinations of hyperparameter values and evaluate the model for each combination. This results in a combination of hyperparameters that yields the best performance based on the chosen evaluation metric.\n",
    "\n",
    "Select the Best Model: Once the grid search is complete, select the hyperparameter combination that achieved the highest performance on the validation fold(s) or the best average performance across all folds. This hyperparameter combination represents the optimal configuration for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8a2c24-ac14-4417-95ab-afd9c4a12dca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fc9aeb8d-cf04-45f2-91fc-a3bbe510ed04",
   "metadata": {},
   "source": [
    "Q2. Describe the difference between grid search cv and randomize search cv, and when might you choose one over the other?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c191eaa5-a7d4-422e-b544-c57cc970742d",
   "metadata": {},
   "source": [
    "Grid Search CV and Randomized Search CV are both hyperparameter optimization techniques used to find the best set of hyperparameters for a machine learning model. Here's the difference between the two:\n",
    "\n",
    "Grid Search CV:\n",
    "\n",
    "Grid Search CV exhaustively searches through all possible combinations of hyperparameters specified in a predefined grid.\n",
    "It evaluates the model's performance for each combination using cross-validation.\n",
    "Grid Search CV is suitable when the hyperparameter search space is relatively small and the number of hyperparameters to tune is limited.\n",
    "It guarantees that all possible combinations will be evaluated, ensuring a comprehensive search through the hyperparameter space.\n",
    "Grid Search CV is computationally expensive, especially when the hyperparameter search space is large or the number of hyperparameters is high.\n",
    "\n",
    "Randomized Search CV:\n",
    "\n",
    "Randomized Search CV randomly samples a specified number of combinations from the hyperparameter search space.\n",
    "It allows you to define a probability distribution or a range for each hyperparameter instead of a predefined grid.\n",
    "Randomized Search CV selects a subset of hyperparameters randomly and evaluates the model's performance using cross-validation.\n",
    "Randomized Search CV is suitable when the hyperparameter search space is large, and a comprehensive search of all combinations is not feasible due to computational limitations.\n",
    "It can be more efficient than Grid Search CV as it explores a smaller subset of the hyperparameter space, reducing the computational cost.\n",
    "However, there is a trade-off between exhaustiveness and efficiency, as Randomized Search CV may not guarantee evaluation of all possible combinations.\n",
    "When to Choose Grid Search CV or Randomized Search CV:\n",
    "\n",
    "Grid Search CV is suitable when:\n",
    "\n",
    "The hyperparameter search space is relatively small.\n",
    "The number of hyperparameters to tune is limited.\n",
    "Sufficient computational resources are available.\n",
    "Exhaustive evaluation of all possible combinations is desired.\n",
    "\n",
    "Randomized Search CV is suitable when:\n",
    "\n",
    "The hyperparameter search space is large or has a wide range of possible values.\n",
    "The number of hyperparameters to tune is large.\n",
    "Computational resources are limited.\n",
    "Exploring a smaller subset of the hyperparameter space is acceptable.\n",
    "An efficient search is desired, even if it does not guarantee evaluation of all possible combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6ceaee-5aba-4b84-a71f-f891e0636f08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3e968755-42f7-404c-8dcb-334386a477ff",
   "metadata": {},
   "source": [
    "Q3. What is data leakage, and why is it a problem in machine learning? Provide an example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb001a7-4993-422e-94e8-3cb11eb46b0a",
   "metadata": {},
   "source": [
    "Data leakage refers to the situation when information from the test or unseen data leaks into the training process, leading to overly optimistic model performance or biased results. It occurs when there is a transfer of information from the target variable or the evaluation metric to the model during training, which would not be available in a real-world scenario.\n",
    "\n",
    "Data leakage is a problem in machine learning because it can lead to inflated performance metrics during model evaluation, giving a false sense of accuracy. This can result in models that perform poorly when deployed on new, unseen data, as they are trained with information that would not be available in practical settings.\n",
    "\n",
    "Here's an example of data leakage:\n",
    "\n",
    "Suppose you are building a credit risk model to predict whether a loan applicant will default on their loan. In the dataset, you have features such as the applicant's credit history, income, and employment status. Additionally, you have a variable called \"Current Loan Status,\" which indicates whether the applicant has already defaulted on a loan.\n",
    "\n",
    "Now, if you include the \"Current Loan Status\" variable in the training process to predict loan default, it would result in data leakage. The reason is that the variable contains information that is not available at the time of making the prediction. Including this variable in the training process would make the model overly optimistic because it would have access to future information (i.e., whether the loan defaults), which would not be available in real-world scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea88b90-fbc3-40a8-a3d6-ea299829de18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "14ede49f-60c6-48b9-b95e-099abbd007ad",
   "metadata": {},
   "source": [
    "Q4. How can you prevent data leakage when building a machine learning model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d940805-7337-4417-b0ab-478027028e69",
   "metadata": {},
   "source": [
    "Preventing data leakage is essential to ensure the integrity and generalizability of machine learning models. Here are some key strategies to prevent data leakage:\n",
    "\n",
    "Splitting Data Properly:\n",
    "\n",
    "Split the dataset into separate and distinct subsets for training, validation, and testing.\n",
    "Ensure that data used for training the model does not overlap with data used for model evaluation.\n",
    "The training set is used exclusively for model training, the validation set for hyperparameter tuning, and the testing set for final evaluation.\n",
    "\n",
    "Feature Selection and Engineering:\n",
    "\n",
    "Perform feature selection and engineering before splitting the data.\n",
    "Avoid using features that leak information about the target variable or evaluation metric.\n",
    "Ensure that the features are based on information available at the time of making predictions.\n",
    "\n",
    "Temporal Validation:\n",
    "\n",
    "If the data has a temporal aspect, ensure that the splitting is done in a time-ordered manner.\n",
    "Use earlier data for training and validation, and reserve the most recent data for testing.\n",
    "This simulates real-world scenarios where predictions are made on future unseen data.\n",
    "\n",
    "Avoiding Target Leakage:\n",
    "\n",
    "Be cautious not to include variables that are derived from the target variable or influenced by it in the model.\n",
    "Variables that are a direct result of the target variable can introduce leakage.\n",
    "For example, including \"future\" information about the target variable in the model.\n",
    "\n",
    "Cross-Validation Techniques:\n",
    "\n",
    "Use appropriate cross-validation techniques such as k-fold cross-validation or stratified cross-validation.\n",
    "Ensure that data leakage is avoided within each fold by performing feature selection and engineering within the training fold.\n",
    "\n",
    "Careful Preprocessing:\n",
    "\n",
    "Be cautious during preprocessing steps such as normalization, imputation, or scaling.\n",
    "Ensure that these steps are performed independently on each fold during cross-validation to prevent information leakage.\n",
    "\n",
    "Expert Knowledge:\n",
    "\n",
    "Seek input from domain experts who can help identify potential sources of data leakage.\n",
    "Their expertise can guide feature selection and engineering processes to ensure adherence to real-world conditions.\n",
    "\n",
    "Vigilant EDA:\n",
    "\n",
    "Conduct exploratory data analysis (EDA) to identify any patterns or variables that may introduce leakage.\n",
    "Be attentive to any unusual relationships or strong correlations between features and the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced7a1cc-0501-4560-8652-92401f995d56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5e28aa77-3de1-432c-84b1-7f01a6bf24f5",
   "metadata": {},
   "source": [
    "Q5. What is a confusion matrix, and what does it tell you about the performance of a classification model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495f84b9-5204-4ab1-ab72-b6e7853e94ab",
   "metadata": {},
   "source": [
    "A confusion matrix is a table that summarizes the performance of a classification model by comparing the predicted class labels with the true class labels of a dataset. It provides a detailed breakdown of the model's predictions and helps evaluate its performance. The confusion matrix is particularly useful in binary classification problems, where there are two classes (e.g., positive and negative) but can also be extended to multi-class problems.\n",
    "\n",
    "A confusion matrix consists of four main elements:\n",
    "\n",
    "True Positives (TP): The number of instances correctly predicted as the positive class.\n",
    "\n",
    "True Negatives (TN): The number of instances correctly predicted as the negative class.\n",
    "\n",
    "False Positives (FP): The number of instances predicted as the positive class but actually belonging to the negative class (Type I error or false alarm).\n",
    "\n",
    "False Negatives (FN): The number of instances predicted as the negative class but actually belonging to the positive class (Type II error or miss).\n",
    "\n",
    "The confusion matrix provides valuable information about the performance of a classification model:\n",
    "\n",
    "Accuracy: It is calculated as (TP + TN) / (TP + TN + FP + FN) and represents the proportion of correct predictions out of all predictions made. However, accuracy alone can be misleading, especially in imbalanced datasets.\n",
    "\n",
    "Precision: It is calculated as TP / (TP + FP) and represents the proportion of true positive predictions out of all positive predictions made. Precision indicates the model's ability to correctly identify positive instances.\n",
    "\n",
    "Recall (Sensitivity or True Positive Rate): It is calculated as TP / (TP + FN) and represents the proportion of true positive predictions out of all actual positive instances. Recall measures the model's ability to correctly identify all positive instances.\n",
    "\n",
    "Specificity (True Negative Rate): It is calculated as TN / (TN + FP) and represents the proportion of true negative predictions out of all actual negative instances. Specificity measures the model's ability to correctly identify all negative instances.\n",
    "\n",
    "F1 Score: It is the harmonic mean of precision and recall, calculated as 2 * (Precision * Recall) / (Precision + Recall). The F1 score provides a balanced measure of the model's performance, considering both precision and recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e772bcce-2099-4966-a381-bbf2f2aec408",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7b7a9dfa-5d9b-4d94-8b6e-d64aea09662c",
   "metadata": {},
   "source": [
    "Q6. Explain the difference between precision and recall in the context of a confusion matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1443a71c-dcdd-48bc-beba-92f873a4382d",
   "metadata": {},
   "source": [
    "Precision and recall are performance metrics derived from a confusion matrix that provide insight into the classification model's performance, particularly in binary classification problems. Here's the difference between precision and recall:\n",
    "\n",
    "Precision:\n",
    "\n",
    "Precision, also known as the positive predictive value, measures the proportion of true positive predictions out of all positive predictions made by the model.\n",
    "It focuses on the correctness of positive predictions and answers the question: \"Of all instances predicted as positive, how many are actually positive?\"\n",
    "Precision is calculated as TP / (TP + FP), where TP represents true positives and FP represents false positives.\n",
    "Precision is a measure of the model's ability to avoid false positives, i.e., to minimize the instances falsely classified as positive.\n",
    "\n",
    "Recall:\n",
    "\n",
    "Recall, also known as sensitivity or true positive rate, measures the proportion of true positive predictions out of all actual positive instances in the dataset.\n",
    "It focuses on the completeness of positive predictions and answers the question: \"Of all actual positive instances, how many were correctly identified by the model?\"\n",
    "Recall is calculated as TP / (TP + FN), where TP represents true positives and FN represents false negatives.\n",
    "Recall is a measure of the model's ability to capture all positive instances and minimize false negatives, i.e., to avoid missing positive instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7d891c-4c35-45bc-adce-c113c64ca2a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "92627ba2-0f46-4c69-a904-ac15763d3aa3",
   "metadata": {},
   "source": [
    "Q7. How can you interpret a confusion matrix to determine which types of errors your model is making?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5902a4b-35f3-44c2-9682-0ceade1e1a1f",
   "metadata": {},
   "source": [
    "Interpreting a confusion matrix helps identify the types of errors a model is making and gain insights into its performance. Here's how you can interpret a confusion matrix to determine the types of errors:\n",
    "\n",
    "True Positives (TP):\n",
    "\n",
    "Instances correctly predicted as the positive class.\n",
    "These are the correct predictions of the positive class, indicating that the model correctly identified positive instances.\n",
    "True Negatives (TN):\n",
    "\n",
    "Instances correctly predicted as the negative class.\n",
    "These are the correct predictions of the negative class, indicating that the model correctly identified negative instances.\n",
    "False Positives (FP):\n",
    "\n",
    "Instances predicted as the positive class but actually belonging to the negative class (Type I error or false alarm).\n",
    "These are instances that the model falsely classified as positive.\n",
    "False positives represent instances where the model predicts a positive outcome but, in reality, the instance belongs to the negative class.\n",
    "False Negatives (FN):\n",
    "\n",
    "Instances predicted as the negative class but actually belonging to the positive class (Type II error or miss).\n",
    "These are instances that the model falsely classified as negative.\n",
    "False negatives represent instances where the model fails to predict a positive outcome and mistakenly classifies the instance as negative.\n",
    "Interpreting these elements in the context of your specific problem can provide insights into the types of errors the model is making:\n",
    "\n",
    "High False Positives (FP):\n",
    "\n",
    "The model is incorrectly predicting positive instances.\n",
    "This indicates that the model has a tendency to make false alarms or classify negative instances as positive.\n",
    "High False Negatives (FN):\n",
    "\n",
    "The model is missing positive instances.\n",
    "This indicates that the model is failing to identify positive instances and mistakenly classifying them as negative.\n",
    "High True Positives (TP):\n",
    "\n",
    "The model is correctly predicting positive instances.\n",
    "This indicates that the model is accurately identifying positive instances.\n",
    "High True Negatives (TN):\n",
    "\n",
    "The model is correctly predicting negative instances.\n",
    "This indicates that the model is accurately identifying negative instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8afbc9cf-a050-49e6-8813-b4387332580d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "479f5d19-b5ca-43a8-9a6b-26ae85ca86ce",
   "metadata": {},
   "source": [
    "Q8. What are some common metrics that can be derived from a confusion matrix, and how are they calculated?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418fc46d-31da-4ae2-a063-8511ab154fab",
   "metadata": {},
   "source": [
    "Several common metrics can be derived from a confusion matrix to evaluate the performance of a classification model. Here are some of the most commonly used metrics and their calculations:\n",
    "\n",
    "Accuracy:\n",
    "\n",
    "Accuracy measures the overall correctness of the model's predictions.\n",
    "It is calculated as (TP + TN) / (TP + TN + FP + FN).\n",
    "Accuracy represents the proportion of correct predictions out of all predictions made by the model.\n",
    "\n",
    "Precision (Positive Predictive Value):\n",
    "\n",
    "Precision measures the proportion of true positive predictions out of all positive predictions made by the model.\n",
    "It is calculated as TP / (TP + FP).\n",
    "Precision indicates the model's ability to avoid false positive predictions.\n",
    "\n",
    "Recall (Sensitivity, True Positive Rate):\n",
    "\n",
    "Recall measures the proportion of true positive predictions out of all actual positive instances in the dataset.\n",
    "It is calculated as TP / (TP + FN).\n",
    "Recall represents the model's ability to capture all positive instances and avoid false negatives.\n",
    "\n",
    "Specificity (True Negative Rate):\n",
    "\n",
    "Specificity measures the proportion of true negative predictions out of all actual negative instances in the dataset.\n",
    "It is calculated as TN / (TN + FP).\n",
    "Specificity represents the model's ability to correctly identify all negative instances and avoid false positives.\n",
    "\n",
    "F1 Score:\n",
    "\n",
    "The F1 score is the harmonic mean of precision and recall.\n",
    "It provides a balanced measure that considers both precision and recall.\n",
    "F1 Score is calculated as 2 * (Precision * Recall) / (Precision + Recall).\n",
    "\n",
    "False Positive Rate (FPR):\n",
    "\n",
    "FPR measures the proportion of false positive predictions out of all actual negative instances.\n",
    "It is calculated as FP / (FP + TN).\n",
    "FPR is the complement of specificity and represents the model's tendency to make false alarms.\n",
    "\n",
    "False Negative Rate (FNR):\n",
    "\n",
    "FNR measures the proportion of false negative predictions out of all actual positive instances.\n",
    "It is calculated as FN / (TP + FN).\n",
    "FNR represents the model's tendency to miss positive instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4478430e-860a-4ae2-ac12-5b86da71f944",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "77db76b3-599a-4796-9562-a97daf732e2d",
   "metadata": {},
   "source": [
    "Q9. What is the relationship between the accuracy of a model and the values in its confusion matrix?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b1f91b-daf9-4490-be1e-80ff11e75fe8",
   "metadata": {},
   "source": [
    "The accuracy of a model is related to the values in its confusion matrix as it is calculated based on the values of true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN) in the confusion matrix. The relationship can be understood as follows:\n",
    "\n",
    "Accuracy:\n",
    "\n",
    "Accuracy is a metric that measures the overall correctness of the model's predictions.\n",
    "It is calculated as (TP + TN) / (TP + TN + FP + FN).\n",
    "Accuracy represents the proportion of correct predictions out of all predictions made by the model.\n",
    "\n",
    "Confusion Matrix:\n",
    "\n",
    "The confusion matrix summarizes the model's predictions and the actual class labels of a dataset.\n",
    "It consists of four elements: TP, TN, FP, and FN.\n",
    "The accuracy metric is calculated using the values from the confusion matrix as follows:\n",
    "\n",
    "True Positives (TP) and True Negatives (TN) contribute positively to the accuracy as they represent correct predictions.\n",
    "False Positives (FP) and False Negatives (FN) contribute negatively to the accuracy as they represent incorrect predictions.\n",
    "The accuracy metric alone does not provide a detailed breakdown of how the model is performing for each class or the types of errors it is making. However, the values in the confusion matrix allow for a more granular analysis. By examining the values in the confusion matrix, you can gain insights into the model's performance for specific classes and types of errors (e.g., false positives or false negatives)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1af9f8d-5f70-48ce-b497-7517a4c2a839",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6e1d5a9e-fd10-4b5f-a5e5-72e6d6ba1522",
   "metadata": {},
   "source": [
    "Q10. How can you use a confusion matrix to identify potential biases or limitations in your machine learning model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f3bd83-de0a-421c-bc64-bf4a1eb2538b",
   "metadata": {},
   "source": [
    "A confusion matrix can be a useful tool to identify potential biases or limitations in a machine learning model. By analyzing the values in the confusion matrix, you can gain insights into how the model is performing for different classes and identify any patterns or discrepancies that indicate biases or limitations. Here are a few ways to utilize the confusion matrix for this purpose:\n",
    "\n",
    "Class Imbalance:\n",
    "\n",
    "Check for significant differences in the number of instances between classes.\n",
    "If the dataset is imbalanced, the model may be biased towards the majority class.\n",
    "Look for a disproportionate number of true positives (TP) or true negatives (TN) in one class compared to the other, indicating potential biases.\n",
    "\n",
    "False Positives and False Negatives:\n",
    "\n",
    "Examine the values of false positives (FP) and false negatives (FN) for each class.\n",
    "False positives suggest the model is incorrectly predicting positive instances, while false negatives indicate instances that are incorrectly classified as negative.\n",
    "Identify classes where the model has a higher rate of false positives or false negatives, as this may indicate biases or limitations.\n",
    "\n",
    "Precision and Recall Disparities:\n",
    "\n",
    "Compare the precision and recall values across different classes.\n",
    "Precision measures the proportion of true positive predictions out of all positive predictions, while recall measures the proportion of true positives out of all actual positive instances.\n",
    "Look for disparities in precision or recall values, as significant differences between classes may indicate biases or limitations.\n",
    "\n",
    "Error Patterns:\n",
    "\n",
    "Analyze the error patterns in the confusion matrix.\n",
    "Look for consistent misclassifications or confusion between specific classes.\n",
    "Identify any patterns or classes where the model consistently struggles to make accurate predictions, which may indicate limitations or biases.\n",
    "\n",
    "External Factors:\n",
    "\n",
    "Consider external factors or data collection processes that may have introduced biases.\n",
    "Biases can arise from biased training data or features that are disproportionately represented or carry inherent biases.\n",
    "Investigate if the model is reflecting those biases by examining the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b08e3d-9637-4df8-b8b2-785257b242df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
